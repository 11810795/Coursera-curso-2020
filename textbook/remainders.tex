\label{section:remainders}

\nobreak In Section~\xrefn{section:finding-taylor-series}, we were finding
Taylor series without worrying at all that what we were finding
actually related to the original function $f$.  We now remedy this
oversight.

Our whole mission in this, Chapter~\xrefn{chapter:taylor-series}, was
to start with a function, then write down a power series converging to
$f$, with the plan that we might finally use that power series to,
say, approximate values of the function, among other things.  What we
have done thus far is \textit{assumed} a function had such a power
series representation, and then deduced that the coefficients are
somehow related to the derivatives of $f$.  So does the power series
converge to the function?

And if the power series we got in
Section~\xrefn{section:finding-taylor-series} does converge to $f$,
how good is the approximation?  I mean, if I just consider the first
$N$ terms of the power series, am I close to $f$ at all?  How close?
Remember our experience with alternating series?  For alternating
series, Theorem~\xrefn{thm:alternating-series-error-bounds} not only
gave a test for convergence: it also bounded how far the $N^{\nth}$
partial sum could be from the true value of the series.  Let's see a
similar sort of ``error estimate'' for power series.

\begin{theorem}[Taylor's theorem]\label{thm:taylors-remainder}
  Suppose that $f$ is defined on some open interval $I = (a-R,a+R)$
  around $a$ and suppose the function $f$ is $(N+1)$-times
  differentiable on $I$, meaning that $\ds f^{(N+1)}(x)$ exists for $x
  \in I$. Then for each $x \neq a$ in $I$ there is a value $z$ between
  $x$ and $a$ so that
$$ 
  f(x) = \sum_{n=0}^N {f^{(n)}(a)\over n!}\,(x-a)^n + 
  {f^{(N+1)}(z)\over (N+1)!}(x-a)^{N+1}. 
$$ 
\end{theorem}
The upshot here is that, by bounding the function $f^{(N+1)}$ on the
interval between $x$ and $a$, we manage to bound the difference
between $f(x)$ and the partial sum.  See
Example~\xrefn{example:approximate-sin} for an example.  But before we
get to the example, let's prove Theorem~\xrefn{thm:taylors-remainder};
the proof perhaps seems unmotivated, since we'll be ``clever'' in
setting things up, but I hope you will be able to follow the argument,
even if you don't trust that you could have created the proof
\textit{ex nihilo.}

\begin{proof}
Define the function $F(t)$ by 
\[
F(t)=\sum_{n=0}^N{f^{(n)}(t)\over n!}\,(x-t)^n + B(x-t)^{N+1} \mbox{ for $t$ between $a$ and $x$.}
\]
Here we have replaced $a$ by $t$ in the first $N+1$ terms of the
Taylor series, and added a carefully chosen term on the end, with $B$
to be determined. Note that
we are temporarily keeping $x$ fixed, so the only variable in this
equation is $t$, and we will be interested
only in $t$ between $a$ and $x$.  If we set $t = a$, then we get
\[
F(a)=\sum_{n=0}^N{f^{(n)}(a)\over n!}\,(x-a)^n + B(x-a)^{N+1}.
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BADBAD START
Set this equal to $f(x)$:
$$f(x)=\sum_{n=0}^N{f^{(n)}(a)\over n!}\,(x-a)^n + B(x-a)^{N+1}.$$
Since $x\not=a$, we can solve this for $B$, which is a
``constant''---it depends on $x$ and $a$ but those are temporarily 
fixed.  Now we
have defined a function $F(t)$ with the property that
$F(a)=f(x)$. Consider also $F(x)$: all terms with a positive power of
$(x-t)$ become zero when we substitute $x$ for $t$, so we are left
with $\ds F(x)=f^{(0)}(x)/0!=f(x)$. So $F(t)$ is a function with the same
value on the endpoints of the interval $[a,x]$. 
By Rolle's theorem, we
know that there is a value $z\in(a,x)$ such that $F'(z)=0$. Let's look
at $F'(t)$. Each term in $F(t)$, except the first term and the extra
term involving $B$, is a product, so to take the derivative we use the
product rule on each of these terms. It will help to write out the
first few terms of the definition:
\begin{align*}
  F(t)=f(t)&+{f^{(1)}(t)\over 1!}(x-t)^1+{f^{(2)}(t)\over 2!}(x-t)^2+
  {f^{(3)}(t)\over 3!}(x-t)^3+\cdots \\
  &+{f^{(N)}(t)\over N!}(x-t)^N+
  B(x-t)^{N+1}. \\
\end{align*}
Now take the derivative:
\begin{align*}
  F'(t) = f'(t) &+ 
  \left({f^{(1)}(t)\over 1!}(x-t)^0(-1)+{f^{(2)}(t)\over
    1!}(x-t)^1\right) \\
  &+\left({f^{(2)}(t)\over 1!}(x-t)^1(-1)+{f^{(3)}(t)\over
    2!}(x-t)^2\right) \\
  &+\left({f^{(3)}(t)\over 2!}(x-t)^2(-1)+{f^{(4)}(t)\over
    3!}(x-t)^3\right)+\dots+ \\
  &+\left({f^{(N)}(t)\over (N-1)!}(x-t)^{N-1}(-1)+{f^{(N+1)}(t)\over
    N!}(x-t)^N\right) \\
  &+B(N+1)(x-t)^N(-1). \\
\end{align*}
Now most of the terms in this expression cancel out,
leaving just
$$F'(t) = {f^{(N+1)}(t)\over N!}(x-t)^N+B(N+1)(x-t)^N(-1).$$
At some $z$, $F'(z)=0$ so
\begin{align*}
  0&={f^{(N+1)}(z)\over N!}(x-z)^N+B(N+1)(x-z)^N(-1) \\
  B(N+1)(x-z)^N&={f^{(N+1)}(z)\over N!}(x-z)^N \\
  B&={f^{(N+1)}(z)\over (N+1)!}. \\
\end{align*}
Now we can write 
$$
  F(t)=\sum_{n=0}^N{f^{(n)}(t)\over n!}\,(x-t)^n + 
  {f^{(N+1)}(z)\over (N+1)!}(x-t)^{N+1}.
$$
Recalling that $F(a)=f(x)$ we get
$$
  f(x)=\sum_{n=0}^N{f^{(n)}(a)\over n!}\,(x-a)^n + 
  {f^{(N+1)}(z)\over (N+1)!}(x-a)^{N+1},
$$
which is what we wanted to show.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BADBAD END

It may not be immediately obvious that this is particularly useful, so
let's look at some examples of Theorem~\xrefn{thm:taylors-remainder}
in action.

\begin{example}\label{example:approximate-sin}
Suppose $x \in [-\pi/2,\pi/2]$.  Find a polynomial approximation for $\sin x$ accurate to $\pm
0.005$. 
\end{example}
\begin{solution}
  Note that if we can compute $\sin x$ for $x\in[-\pi/2,\pi/2]$, then
  we can compute $\sin x$ for all $x$.  So what we are asking for here
  is actually quite general!  Once we figure out how to calculate
  $\sin x$ for $x \in [-\pi/2,\pi/2]$, we can shift any other value of
  $x$ into that interval, and thereby compute $\sin x$.

  Let's get started.  From Theorem~\xrefn{thm:taylors-remainder}, we
  have that
  \[
  \sin x= \sum_{n=0}^N{f^{(n)}(a)\over n!}\,(x-a)^n + {f^{(N+1)}(z)\over
    (N+1)!}(x-a)^{N+1} \mbox{ for some $z$ between $a$ and $x$.}
  \]
  What can we say about the size of that ``error'' term?  In other words, how big could
  $${f^{(N+1)}(z)\over (N+1)!}(x-a)^{N+1}$$
  possibly be when $z$ is between $x$ and $a$?  Every derivative of
  $\sin x$ is either $\pm\sin x$ or $\pm\cos x$, so $\ds
  |f^{(N+1)}(z)| \leq 1$. The factor $\ds (x-a)^{N+1}$ is a bit more
  difficult, since $x-a$ could be quite large. To simplify matters,
  let's pick $a=0$.  We need to pick $N$ so that 
  \[
  \left|{x^{N+1}\over (N+1)!}\right|< 0.005.
  \]
  Since we are only considering the case when $x \in [-\pi/2,\pi/2]$, we have
  \[
  \left|{x^{N+1}\over (N+1)!}\right|<{2^{N+1}\over (N+1)!}.
  \]
  The quantity on the right decreases with increasing $N$, so all we
  need to do is find an $N$ so that 
  $${2^{N+1}\over (N+1)!}<0.005.$$
  A little trial and error shows that $N=8$ works, 
  and in fact $\ds 2^{9}/9!<0.0015$.  Consequently,
  \begin{align*}
    \sin x &=\sum_{n=0}^8{f^{(n)}(0)\over n!}\,x^n \pm 0.0015 \\
    &=x-{x^3\over 6}+{x^5\over 120}-{x^7\over 5040}\pm 0.0015. \\
  \end{align*}
  Figure~\xrefn{fig:sine-approximation} shows the graphs of $\sin x$ and
  and the approximation on $[0,3\pi/2]$. As $x$ gets larger, the
  approximation heads to negative infinity very quickly, since it is
  essentially acting like $\ds -x^7$.
\end{solution}

\begin{marginfigure}[0in]

\begin{tikzpicture}
	\begin{axis}[
            domain=-4:4,
            ymax=1.25,
            ymin=-1.25,
            xmin=-4,
            xmax=4,
            axis lines =middle, xlabel={$x$}, ylabel=$y$,
            every axis y label/.style={at=(current axis.above origin),anchor=south},
            every axis x label/.style={at=(current axis.right of origin),anchor=west}
          ]
          \addplot [penColor, smooth, very thick,domain=(-4:4)] {sin(deg(x))};
          \node [anchor=south west,penColor] at (axis cs:1.25,1.0) {$f(x) = \sin x$};

          \addplot [penColor2, smooth, dotted, domain=(-4:4)] {x};

          \addplot [penColor2, smooth, dashed, domain=(-4:4)] {x - x*x*x/6};

          \addplot [penColor2, samples=100, smooth, domain=(-4:4)] {x - x*x*x/6 + x*x*x*x*x/120};

          %\draw[penColor2] (axis cs:0,0) rectangle (axis cs:1,1);
          %\node [anchor=north,penColor2] at (axis cs:0.5,1) {$a_1$};
          %\draw[penColor2] (axis cs:1,0) rectangle (axis cs:2,0.25);
          %\node [anchor=north,penColor2] at (axis cs:1.5,0.25) {$a_2$};
          %\draw[penColor2] (axis cs:2,0) rectangle (axis cs:3,0.11111111);
          %\node [anchor=north,penColor2] at (axis cs:2.5,0.11111111) {$a_3$};
          %\draw[penColor2] (axis cs:3,0) rectangle (axis cs:4,0.0625);
          %\node [anchor=south,penColor2,yshift=2pt] at (axis cs:3.5,0.0625) {$a_4$};
          %\draw[penColor2] (axis cs:4,0) rectangle (axis cs:5,0.04);
          %\node [anchor=south,penColor2,yshift=2pt] at (axis cs:4.5,0.04) {$a_5$};
          %\draw[penColor2] (axis cs:5,0) rectangle (axis cs:6,0.027777777);

        \end{axis}
\end{tikzpicture}
\caption{A plot of $f(x) = \sin x$ with a thick line, placed alongside
  the dotted line $y = x$, a dashed plot of $y = x - \frac{x^3}{6}$,
  and a thin solid plot of $y = x - \frac{x^3}{6} + \frac{x^5}{120}$.
  Note how successively higher partial sums of the Taylor series are
  hugging the graph of $\sin x$ increasingly well.}
\label{fig:sine-approximation}
\end{marginfigure}

We can extract a bit more information from this example. If we do not
restrict the value of $x$ to lie in the interval $x \in
[-\pi/2,\pi/2]$, we still know that
$$
  \left|{f^{(N+1)}(z)\over (N+1)!}x^{N+1}\right|\le 
  \left|{x^{N+1}\over (N+1)!}\right|
$$
because the derivative $f^{(N+1)}(z)$ is either $\pm \sin z$ or $\pm
\cos z$.  We can use this to prove the following result.
\begin{theorem}
For all real numbers $x$,
$$
\sin x = \sum_{n=0}^\infty \frac{(-1)^n}{(2n+1)!} x^{2n+1}.
$$
\end{theorem}
\begin{proof}
  If we can show that 
  $$
  \lim_{N\to\infty} \left|{x^{N+1}\over (N+1)!}\right|=0
  $$
  for each $x$, then we know that the error term
  $$
  \left|{f^{(N+1)}(z)\over (N+1)!}x^{N+1}\right|\le 
  \left|{x^{N+1}\over (N+1)!}\right|,
  $$
  is, in the limit, zero, and so we may conclude that
  \begin{align*}
  \sin x&=\sum_{n=0}^\infty{f^{(n)}(0)\over n!}\,x^n \\
  &= \sum_{n=0}^\infty (-1)^n{x^{2n+1}\over (2n+1)!}.
  \end{align*}
  In other words, we will conclude that the sine function is actually
  equal to its Taylor series around zero for all $x$. But how can we prove
  that the limit is zero?

  Suppose that $N$ is larger than $|x|$, and let $M$ be the largest
  integer less than $|x|$.  If $M = 0$, then $x^{N+1}$ is small, and
  even smaller after dividing by $(N+1)!$.  On the other hand, if $M >
  0$, then just a bit more work is called for.  We compute that
  \begin{align*}
    {|x^{N+1}|\over (N+1)!} &= {|x|\over N+1}{|x|\over N}{|x|\over N-1}\cdots
    {|x|\over M+1}{|x|\over M}{|x|\over M-1}\cdots {|x|\over 2}{|x|\over 1} \\
    &\le {|x|\over N+1}\cdot 1\cdot 1\cdots 1\cdot
    {|x|\over M}{|x|\over M-1}\cdots {|x|\over 2}{|x|\over 1} \\
    &={|x|\over N+1}{|x|^M\over M!}.
  \end{align*}
  The quantity $|x|^M/ M!$ is a constant, so 
  $$
  \lim_{N\to\infty} {|x|\over N+1}{|x|^M\over M!} = 0
  $$
  and by squeezing via Theorem~\xrefn{thm:squeeze-theorem-for-sequences},
  $$
  \lim_{N\to\infty} \left|{x^{N+1}\over (N+1)!}\right|=0
  $$
  as desired.
\end{proof}

Essentially the same argument works for $\cos x$, which yields the following.
\begin{theorem}\label{thm:cosine-is-analytic}
  For all real numbers $x$,
  $$
  \cos x = \sum_{n=0}^\infty \frac{(-1)^n}{(2n)!} x^{2n}.
  $$
\end{theorem}

And similarly, the function $e^x$ is \defnword{real analytic},
meaning that the Taylor series for $e^x$ converges to $e^x$.
\begin{theorem}\label{thm:exp-is-analytic}
  For all real numbers $x$,
  $$
  e^x = \sum_{n=0}^\infty \frac{1}{n!} x^{n}.
  $$  
\end{theorem}


\begin{example} Find a polynomial approximation for $\ds e^x$ near $x=2$
accurate to $\pm
0.005$. 
\end{example}
\begin{solution}
From Taylor's theorem:
$$
  e^x= \sum_{n=0}^N{e^2\over n!}\,(x-2)^n + 
  {e^z\over (N+1)!}(x-2)^{N+1},
$$
since $\ds f^{(n)}(x)=e^x$ for all $n$. We are interested in $x$ near
2, and we need to control the size of $\ds |(x-2)^{N+1}|$, so we may
as well specify that $|x-2|\le 1$, meaning $x\in[1,3]$.  Also
$$\left|{e^z\over (N+1)!}\right|\le {e^3\over (N+1)!},$$
so we need to find an $N$ that makes $\ds e^3/(N+1)!\le 0.005$. This time
$N=5$ makes $\ds e^3/(N+1)!< 0.0015$, so the approximating polynomial is
$$
  e^x=e^2+e^2(x-2)+{e^2\over2}(x-2)^2+{e^2\over6}(x-2)^3+
  {e^2\over24}(x-2)^4+{e^2\over120}(x-2)^5
  \pm 0.0015.
$$
This presents an additional problem for approximation, since we also
need to approximate $\ds e^2$, and any approximation we use will increase
the error, but we will not pursue this complication.
\end{solution}

Note well that in these examples we found polynomials of a certain
accuracy only on a small interval, even though the series for $\sin x$
and $\ds e^x$ converge for all $x$.  This is usually how things work
out.  To get the same accuracy on a larger interval, what can you do?
Use more terms!

\begin{exercises}
  \begin{exercise} Find a polynomial approximation for $\cos x$ on
    $[0,\pi]$, accurate to $\ds \pm 10^{-3}$
\begin{answer} $\ds 1-{x^2\over2}+{x^4\over24}-{x^6\over 720}
+\cdots+{x^{12}\over 12!}$
\end{answer}\end{exercise}

\begin{exercise} How many terms of the series for $\log x$ centered at
  1 are required so that the guaranteed error on $[1/2,3/2]$ is at
  most $\ds 10^{-3}$? What if the interval is instead $[1,3/2]$?
\begin{answer} $1000$; $8$ 
\end{answer}\end{exercise}

\begin{exercise} Find the first three nonzero terms in the Taylor
  series for $\tan x$ on $[-\pi/4,\pi/4]$, and compute the guaranteed
  error term as given by Taylor's theorem. (You may want to use Sage
  or a similar aid.)
\begin{answer} $\ds x+{x^3\over3}+{2x^5\over15}$, error $\pm 1.27$.
\end{answer}\end{exercise}

\begin{exercise} Prove Theorem~\xrefn{thm:cosine-is-analytic}, that
  is, show that $\cos x$ is equal to its Taylor series for all $x$ by
  showing that the limit of the error term is zero as $N$ approaches
  infinity.
\end{exercise}

\begin{exercise} Prove Theorem~\xrefn{thm:exp-is-analytic}, that is,
  show that $\ds e^x$ is equal to its Taylor series for all $x$ by
  showing that the limit of the error term is zero as $N$ approaches
  infinity.
\end{exercise}

\end{exercises}
